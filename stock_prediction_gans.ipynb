{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction Using GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The goal is to take a number of past and conscutive stock prices and predict the price of the next day. For example, if we take a time series from 1 to 't', we will predict the t+1 price.\n",
    "One of the greatest strengths of a GAN is the fact that it can extract features in a sophisticated manner and replicate the distribution of the real data.\n",
    "\n",
    "The proposed idea is to use Generative Adversarial Network (GAN) with Convolutional Neural Network (CNN) as the discriminator and Long Short-Term memory (LSTM) as the genertor for predicting the stock price on t+1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The idea is to start the project with a simple set of data and keep adding features and data sources to help the model extract useful information of the market.\n",
    "\n",
    "As the first step, we will start with just getting the stock price information from `finnhub.io`\n",
    "We will split the data to train/test sets in a manner that is acceptable to for time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "Our strategy will be to split the data into batches of 16 stock prices and then feed the first 15 of them to the Generator and have the LSTM model to predict the 16th day. But, before feeding the data, we ill normalize each batch.\n",
    "\n",
    "Another strategy is to feed a Gaussian distribution to the Generator and have it learn the 16 prices.\n",
    "The first strategy should be a better fit for us because we want to control the distribution of the first 15 days and not feed randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_date_to_timestamp(date_string: str) -> str:\n",
    "    date = datetime.datetime.strptime(date_string, \"%m/%d/%Y\")\n",
    "    return datetime.datetime.timestamp(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_time = from_date_to_timestamp(\"8/19/2020\")\n",
    "from_time =from_date_to_timestamp(\"8/19/2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    'https://finnhub.io/api/v1/stock/candle?symbol=AAPL&resolution=1&from={}&to={}'.format(from_time, to_time),\n",
    "#     params={'q': 'requests+language:python'},\n",
    "    headers={'X-Finnhub-Token': 'bt0udmn48v6qcviaikf0'}\n",
    ")\n",
    "\n",
    "# response = requests.get(/stock/candle?symbol=AAPL&resolution=1&from=1572651390&to=1572910590&token=bt0udmn48v6qcviaikf0')\n",
    "\n",
    "# View the new `text-matches` array which provides information\n",
    "# about your search term within the results\n",
    "json_response = response.json()\n",
    "data = pd.DataFrame(data=json_response)\n",
    "data.columns = [\"Close\", \"High\", \"Low\", \"Open\", \"Status\", \"DateTime\", \"Volume\"]\n",
    "data.drop(['Status'], axis=1, inplace=True) #not needed column\n",
    "# repository = json_response['items'][0]\n",
    "# print(f'Text matches: {repository[\"text_matches\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "data['DateTime'] = data['DateTime'].apply(lambda t: datetime.datetime.fromtimestamp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO partition the data in year and month and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317.460</td>\n",
       "      <td>317.4699</td>\n",
       "      <td>317.08</td>\n",
       "      <td>317.120</td>\n",
       "      <td>2020-05-12 17:32:00</td>\n",
       "      <td>85768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317.490</td>\n",
       "      <td>317.5700</td>\n",
       "      <td>317.39</td>\n",
       "      <td>317.420</td>\n",
       "      <td>2020-05-12 17:33:00</td>\n",
       "      <td>92240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317.560</td>\n",
       "      <td>317.6098</td>\n",
       "      <td>317.42</td>\n",
       "      <td>317.480</td>\n",
       "      <td>2020-05-12 17:34:00</td>\n",
       "      <td>54167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317.480</td>\n",
       "      <td>317.5900</td>\n",
       "      <td>317.45</td>\n",
       "      <td>317.555</td>\n",
       "      <td>2020-05-12 17:35:00</td>\n",
       "      <td>45673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317.446</td>\n",
       "      <td>317.5401</td>\n",
       "      <td>317.40</td>\n",
       "      <td>317.500</td>\n",
       "      <td>2020-05-12 17:36:00</td>\n",
       "      <td>44307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Close      High     Low     Open            DateTime  Volume\n",
       "0  317.460  317.4699  317.08  317.120 2020-05-12 17:32:00   85768\n",
       "1  317.490  317.5700  317.39  317.420 2020-05-12 17:33:00   92240\n",
       "2  317.560  317.6098  317.42  317.480 2020-05-12 17:34:00   54167\n",
       "3  317.480  317.5900  317.45  317.555 2020-05-12 17:35:00   45673\n",
       "4  317.446  317.5401  317.40  317.500 2020-05-12 17:36:00   44307"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def batch_data(prices, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    n_batches = len(prices)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    prices = prices[:n_batches*batch_size]\n",
    "    \n",
    "    features, target = [], []\n",
    "    \n",
    "    for idx in range(0, (len(prices) - sequence_length)):\n",
    "        features.append(prices[idx : idx + sequence_length])\n",
    "        target.append(prices[idx + sequence_length])\n",
    "    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(features)), torch.from_numpy(np.asarray(target)))\n",
    "    data_loader = DataLoader(data, shuffle= False, batch_size = batch_size)\n",
    "    # return a dataloader\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 16])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
      "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
      "        [ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "        [ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
      "        [ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
      "        [ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
      "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
      "        [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
      "        [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
      "        [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
      "        [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
      "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
      "        [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],\n",
      "        [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37],\n",
      "        [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
      "        [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41],\n",
      "        [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42],\n",
      "        [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n",
      "        [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
      "        [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46],\n",
      "        [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
      "        [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]])\n",
      "\n",
      "torch.Size([34])\n",
      "tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_prices = range(50)\n",
    "t_loader = batch_data(test_prices, sequence_length=16, batch_size=50)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeBatch(data_loader):\n",
    "    min_val = data_loader.min(1, keepdim=True)[0]\n",
    "    max_val = data_loader.max(1, keepdim=True)[0]\n",
    "    data_loader = data_loader.double()\n",
    "    data_loader -= min_val\n",
    "    data_loader /= max_val\n",
    "    return data_loader, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "        [ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       "        [ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "        [ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       "        [ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
       "        [ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
       "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
       "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
       "        [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
       "        [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "        [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
       "        [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
       "        [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
       "        [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
       "        [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
       "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       "        [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],\n",
       "        [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37],\n",
       "        [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38],\n",
       "        [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "        [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n",
       "        [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41],\n",
       "        [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42],\n",
       "        [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n",
       "        [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
       "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
       "        [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46],\n",
       "        [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
       "        [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0667, 0.1333, 0.2000, 0.2667, 0.3333, 0.4000, 0.4667, 0.5333,\n",
       "          0.6000, 0.6667, 0.7333, 0.8000, 0.8667, 0.9333, 1.0000],\n",
       "         [0.0000, 0.0625, 0.1250, 0.1875, 0.2500, 0.3125, 0.3750, 0.4375, 0.5000,\n",
       "          0.5625, 0.6250, 0.6875, 0.7500, 0.8125, 0.8750, 0.9375],\n",
       "         [0.0000, 0.0588, 0.1176, 0.1765, 0.2353, 0.2941, 0.3529, 0.4118, 0.4706,\n",
       "          0.5294, 0.5882, 0.6471, 0.7059, 0.7647, 0.8235, 0.8824],\n",
       "         [0.0000, 0.0556, 0.1111, 0.1667, 0.2222, 0.2778, 0.3333, 0.3889, 0.4444,\n",
       "          0.5000, 0.5556, 0.6111, 0.6667, 0.7222, 0.7778, 0.8333],\n",
       "         [0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684, 0.4211,\n",
       "          0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895],\n",
       "         [0.0000, 0.0500, 0.1000, 0.1500, 0.2000, 0.2500, 0.3000, 0.3500, 0.4000,\n",
       "          0.4500, 0.5000, 0.5500, 0.6000, 0.6500, 0.7000, 0.7500],\n",
       "         [0.0000, 0.0476, 0.0952, 0.1429, 0.1905, 0.2381, 0.2857, 0.3333, 0.3810,\n",
       "          0.4286, 0.4762, 0.5238, 0.5714, 0.6190, 0.6667, 0.7143],\n",
       "         [0.0000, 0.0455, 0.0909, 0.1364, 0.1818, 0.2273, 0.2727, 0.3182, 0.3636,\n",
       "          0.4091, 0.4545, 0.5000, 0.5455, 0.5909, 0.6364, 0.6818],\n",
       "         [0.0000, 0.0435, 0.0870, 0.1304, 0.1739, 0.2174, 0.2609, 0.3043, 0.3478,\n",
       "          0.3913, 0.4348, 0.4783, 0.5217, 0.5652, 0.6087, 0.6522],\n",
       "         [0.0000, 0.0417, 0.0833, 0.1250, 0.1667, 0.2083, 0.2500, 0.2917, 0.3333,\n",
       "          0.3750, 0.4167, 0.4583, 0.5000, 0.5417, 0.5833, 0.6250],\n",
       "         [0.0000, 0.0400, 0.0800, 0.1200, 0.1600, 0.2000, 0.2400, 0.2800, 0.3200,\n",
       "          0.3600, 0.4000, 0.4400, 0.4800, 0.5200, 0.5600, 0.6000],\n",
       "         [0.0000, 0.0385, 0.0769, 0.1154, 0.1538, 0.1923, 0.2308, 0.2692, 0.3077,\n",
       "          0.3462, 0.3846, 0.4231, 0.4615, 0.5000, 0.5385, 0.5769],\n",
       "         [0.0000, 0.0370, 0.0741, 0.1111, 0.1481, 0.1852, 0.2222, 0.2593, 0.2963,\n",
       "          0.3333, 0.3704, 0.4074, 0.4444, 0.4815, 0.5185, 0.5556],\n",
       "         [0.0000, 0.0357, 0.0714, 0.1071, 0.1429, 0.1786, 0.2143, 0.2500, 0.2857,\n",
       "          0.3214, 0.3571, 0.3929, 0.4286, 0.4643, 0.5000, 0.5357],\n",
       "         [0.0000, 0.0345, 0.0690, 0.1034, 0.1379, 0.1724, 0.2069, 0.2414, 0.2759,\n",
       "          0.3103, 0.3448, 0.3793, 0.4138, 0.4483, 0.4828, 0.5172],\n",
       "         [0.0000, 0.0333, 0.0667, 0.1000, 0.1333, 0.1667, 0.2000, 0.2333, 0.2667,\n",
       "          0.3000, 0.3333, 0.3667, 0.4000, 0.4333, 0.4667, 0.5000],\n",
       "         [0.0000, 0.0323, 0.0645, 0.0968, 0.1290, 0.1613, 0.1935, 0.2258, 0.2581,\n",
       "          0.2903, 0.3226, 0.3548, 0.3871, 0.4194, 0.4516, 0.4839],\n",
       "         [0.0000, 0.0312, 0.0625, 0.0938, 0.1250, 0.1562, 0.1875, 0.2188, 0.2500,\n",
       "          0.2812, 0.3125, 0.3438, 0.3750, 0.4062, 0.4375, 0.4688],\n",
       "         [0.0000, 0.0303, 0.0606, 0.0909, 0.1212, 0.1515, 0.1818, 0.2121, 0.2424,\n",
       "          0.2727, 0.3030, 0.3333, 0.3636, 0.3939, 0.4242, 0.4545],\n",
       "         [0.0000, 0.0294, 0.0588, 0.0882, 0.1176, 0.1471, 0.1765, 0.2059, 0.2353,\n",
       "          0.2647, 0.2941, 0.3235, 0.3529, 0.3824, 0.4118, 0.4412],\n",
       "         [0.0000, 0.0286, 0.0571, 0.0857, 0.1143, 0.1429, 0.1714, 0.2000, 0.2286,\n",
       "          0.2571, 0.2857, 0.3143, 0.3429, 0.3714, 0.4000, 0.4286],\n",
       "         [0.0000, 0.0278, 0.0556, 0.0833, 0.1111, 0.1389, 0.1667, 0.1944, 0.2222,\n",
       "          0.2500, 0.2778, 0.3056, 0.3333, 0.3611, 0.3889, 0.4167],\n",
       "         [0.0000, 0.0270, 0.0541, 0.0811, 0.1081, 0.1351, 0.1622, 0.1892, 0.2162,\n",
       "          0.2432, 0.2703, 0.2973, 0.3243, 0.3514, 0.3784, 0.4054],\n",
       "         [0.0000, 0.0263, 0.0526, 0.0789, 0.1053, 0.1316, 0.1579, 0.1842, 0.2105,\n",
       "          0.2368, 0.2632, 0.2895, 0.3158, 0.3421, 0.3684, 0.3947],\n",
       "         [0.0000, 0.0256, 0.0513, 0.0769, 0.1026, 0.1282, 0.1538, 0.1795, 0.2051,\n",
       "          0.2308, 0.2564, 0.2821, 0.3077, 0.3333, 0.3590, 0.3846],\n",
       "         [0.0000, 0.0250, 0.0500, 0.0750, 0.1000, 0.1250, 0.1500, 0.1750, 0.2000,\n",
       "          0.2250, 0.2500, 0.2750, 0.3000, 0.3250, 0.3500, 0.3750],\n",
       "         [0.0000, 0.0244, 0.0488, 0.0732, 0.0976, 0.1220, 0.1463, 0.1707, 0.1951,\n",
       "          0.2195, 0.2439, 0.2683, 0.2927, 0.3171, 0.3415, 0.3659],\n",
       "         [0.0000, 0.0238, 0.0476, 0.0714, 0.0952, 0.1190, 0.1429, 0.1667, 0.1905,\n",
       "          0.2143, 0.2381, 0.2619, 0.2857, 0.3095, 0.3333, 0.3571],\n",
       "         [0.0000, 0.0233, 0.0465, 0.0698, 0.0930, 0.1163, 0.1395, 0.1628, 0.1860,\n",
       "          0.2093, 0.2326, 0.2558, 0.2791, 0.3023, 0.3256, 0.3488],\n",
       "         [0.0000, 0.0227, 0.0455, 0.0682, 0.0909, 0.1136, 0.1364, 0.1591, 0.1818,\n",
       "          0.2045, 0.2273, 0.2500, 0.2727, 0.2955, 0.3182, 0.3409],\n",
       "         [0.0000, 0.0222, 0.0444, 0.0667, 0.0889, 0.1111, 0.1333, 0.1556, 0.1778,\n",
       "          0.2000, 0.2222, 0.2444, 0.2667, 0.2889, 0.3111, 0.3333],\n",
       "         [0.0000, 0.0217, 0.0435, 0.0652, 0.0870, 0.1087, 0.1304, 0.1522, 0.1739,\n",
       "          0.1957, 0.2174, 0.2391, 0.2609, 0.2826, 0.3043, 0.3261],\n",
       "         [0.0000, 0.0213, 0.0426, 0.0638, 0.0851, 0.1064, 0.1277, 0.1489, 0.1702,\n",
       "          0.1915, 0.2128, 0.2340, 0.2553, 0.2766, 0.2979, 0.3191],\n",
       "         [0.0000, 0.0208, 0.0417, 0.0625, 0.0833, 0.1042, 0.1250, 0.1458, 0.1667,\n",
       "          0.1875, 0.2083, 0.2292, 0.2500, 0.2708, 0.2917, 0.3125]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[ 0],\n",
       "         [ 1],\n",
       "         [ 2],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33]]),\n",
       " tensor([[15],\n",
       "         [16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35],\n",
       "         [36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39],\n",
       "         [40],\n",
       "         [41],\n",
       "         [42],\n",
       "         [43],\n",
       "         [44],\n",
       "         [45],\n",
       "         [46],\n",
       "         [47],\n",
       "         [48]]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeBatch(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import problem_unittests as tests\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, \n",
    "                           kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    # append conv layer\n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        # append batchnorm layer\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "     \n",
    "    # using Sequential container\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Discriminator Module\n",
    "        :param conv_dim: The depth of the first convolutional layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        # 32x32 input\n",
    "        self.conv1 = conv(1, conv_dim, 4, batch_norm=False) # first layer, no batch_norm\n",
    "        # 16x16 out\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        # 8x8 out\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        # 4x4 out\n",
    "        \n",
    "        # final, fully-connected layer\n",
    "        self.fc = nn.Linear(conv_dim*4*4*4, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: Discriminator logits; the output of the neural network\n",
    "        \"\"\"\n",
    "        # define feedforward behavior\n",
    "        # all hidden layers + leaky relu activation\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(-1, self.conv_dim*4*4*4)\n",
    "        \n",
    "        # final output layer\n",
    "        x = self.fc(x)            \n",
    "        return x\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "# \"\"\"\n",
    "# tests.test_discriminator(Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The input to the generator is a 15 data points of real data into the LSTM model which outputs the following data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param stock_data_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param stock_prediction_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = 1\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=1, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        #dropout layer before FC layer\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #fully-connected output layers\n",
    "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "#         self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "#                             torch.zeros(1,1,self.hidden_layer_size))\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"       \n",
    "        \n",
    "        #LSTM\n",
    "        r_output, hidden = self.lstm(input_seq.view(len(input_seq) ,1, -1), hidden)\n",
    "        out_dropout = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "#         out_before_fc = r_output.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out_before_fc = r_output.contiguous().view(-1, self.hidden_dim)        \n",
    "\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        batch_size = nn_input.size(0)\n",
    "        output = out_after_fc.view(batch_size, -1, self.hidden_layer_size)\n",
    "        # get last batch\n",
    "        out = output[:, -1]\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_layer_size).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "# \"\"\"\n",
    "# tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    \"\"\"\n",
    "    Applies initial weights to certain layers in a model .\n",
    "    The weights are taken from a normal distribution \n",
    "    with mean = 0, std dev = 0.02.\n",
    "    :param m: A module or layer in a network    \n",
    "    \"\"\"\n",
    "    # classname will be something like:\n",
    "    # `Conv`, `BatchNorm2d`, `Linear`, etc.\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    # TODO: Apply initial weights to convolutional and linear layers\n",
    "    \n",
    "    if classname in ['Conv', 'Linear']:\n",
    "        nn.init.normal_(m.weight.data, mean = 0, std = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "def build_network(d_conv_dim, g_conv_dim, z_size):\n",
    "    # define discriminator and generator\n",
    "    D = Discriminator(d_conv_dim)\n",
    "#     stock_data_size, stock_prediction_size, embedding_dim, hidden_dim, n_layers, dropout=0.5\n",
    "    G = Generator(16, 100, 1, 0.5)\n",
    "\n",
    "    # initialize model weights\n",
    "    D.apply(weights_init_normal)\n",
    "    G.apply(weights_init_normal)\n",
    "\n",
    "    print(D)\n",
    "    print()\n",
    "    print(G)\n",
    "    \n",
    "    return D, G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (lstm): LSTM(16, 100, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model hyperparams\n",
    "d_conv_dim = 16\n",
    "g_conv_dim = 16\n",
    "z_size = 256\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "D, G = build_network(d_conv_dim, g_conv_dim, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Training on GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator and Generator Losses\n",
    "\n",
    "d_loss = d_real_loss + d_fake_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out):\n",
    "    '''Calculates how close discriminator outputs are to being real.\n",
    "       param, D_out: discriminator logits\n",
    "       return: real loss'''\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.ones(batch_size) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    '''Calculates how close discriminator outputs are to being fake.\n",
    "       param, D_out: discriminator logits\n",
    "       return: fake loss'''\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizers for the discriminator D and generator G\n",
    "\n",
    "# params\n",
    "lr = 0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D, G, n_epochs, print_every=50):\n",
    "    '''Trains adversarial networks for some number of epochs\n",
    "       param, D: the discriminator network\n",
    "       param, G: the generator network\n",
    "       param, n_epochs: number of epochs to train for\n",
    "       param, print_every: when to print and record the models' losses\n",
    "       return: D and G losses'''\n",
    "    \n",
    "    # move models to GPU\n",
    "    if train_on_gpu:\n",
    "        D.cuda()\n",
    "        G.cuda()\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "\n",
    "    # Get some fixed data for sampling. These are images that are held\n",
    "    # constant throughout training, and allow us to inspect the model's performance\n",
    "    sample_size=16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    # move z to GPU if available\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "\n",
    "    # epoch training loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # batch training loop\n",
    "        for batch_i, (real_images, _) in enumerate(celeba_train_loader):\n",
    "\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = scale(real_images)\n",
    "\n",
    "            # ===============================================\n",
    "            #         YOUR CODE HERE: TRAIN THE NETWORKS\n",
    "            # ===============================================\n",
    "            \n",
    "            # 1. Train the discriminator on real and fake images\n",
    "            d_optimizer.zero_grad()\n",
    "        \n",
    "            # 1. Train with real images\n",
    "\n",
    "            # Compute the discriminator losses on real images \n",
    "            if train_on_gpu:\n",
    "                real_images = real_images.cuda()\n",
    "\n",
    "            D_real = D(real_images)\n",
    "            d_real_loss = real_loss(D_real)\n",
    "\n",
    "            # 2. Train with fake images\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            # move x to GPU, if available\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)\n",
    "\n",
    "            # Compute the discriminator losses on fake images            \n",
    "            D_fake = D(fake_images)\n",
    "            d_fake_loss = fake_loss(D_fake)\n",
    "\n",
    "            # add up loss and perform backprop\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "\n",
    "            # 2. Train the generator with an adversarial loss\n",
    "            g_optimizer.zero_grad()\n",
    "        \n",
    "            # 1. Train with fake images and flipped labels\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)\n",
    "\n",
    "            # Compute the discriminator losses on fake images \n",
    "            # using flipped labels!\n",
    "            D_fake = D(fake_images)\n",
    "            g_loss = real_loss(D_fake) # use real loss to flip labels\n",
    "\n",
    "            # perform backprop\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # ===============================================\n",
    "            #              END OF YOUR CODE\n",
    "            # ===============================================\n",
    "\n",
    "            # Print some loss stats\n",
    "            if batch_i % print_every == 0:\n",
    "                # append discriminator loss and generator loss\n",
    "                losses.append((d_loss.item(), g_loss.item()))\n",
    "                # print discriminator and generator loss\n",
    "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "\n",
    "        ## AFTER EACH EPOCH##    \n",
    "        # this code assumes your generator is named G, feel free to change the name\n",
    "        # generate and save sample, fake images\n",
    "        G.eval() # for generating samples\n",
    "        samples_z = G(fixed_z)\n",
    "        samples.append(samples_z)\n",
    "        G.train() # back to training mode\n",
    "\n",
    "    # Save training generator samples\n",
    "    with open('train_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    # finally return losses\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
